---
title: ML System Papers and Resources
date: 2025-06-08 
categories: [Deep Learing, ML Systems, ML Acceleration]
tags: [gpu, deep learning, ml_systems]     # TAG names should always be lowercase
---


# ML System Papers

## June 2025
* [Truncated Proximal Policy Optimization](https://arxiv.org/abs/2506.15050)
* [MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models](http://arxiv.org/abs/2506.14435)
* [SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation](https://arxiv.org/abs/2506.18349)
* [CommVQ: Commutative Vector Quantization for KV Cache Compression](http://arxiv.org/abs/2506.18879)
* [DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster](https://arxiv.org/abs/2506.21263)
* [Scaling Speculative Decoding with Lookahead Reasoning](https://arxiv.org/abs/2506.19830)
* [OAgents: An Empirical Study of Building Effective Agents](https://arxiv.org/abs/2506.15741)
* [Efficient RL Training - Optimizing Memory Usage in verl](https://hebiao064.github.io/rl-memory-management)
* [FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving](https://arxiv.org/abs/2501.01005)
* [Hardware-Efficient Attention for Fast Decoding](https://arxiv.org/abs/2505.21487)
* [PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models](https://arxiv.org/abs/2506.16054)
* [Scaling Test-time Compute for LLM Agents](https://arxiv.org/abs/2506.12928)
* [MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention](https://arxiv.org/abs/2506.13585)
* [Magistral](https://arxiv.org/abs/2506.10910)
* [Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/abs/2506.06395)
* [Seedance 1.0: Exploring the Boundaries of Video Generation Models](https://arxiv.org/abs/2506.09113)
* [NoLoCo: No-all-reduce Low Communication Training Method for Large Models](https://arxiv.org/abs/2506.10911)
* [Reinforcement Pre-Training](https://arxiv.org/abs/2506.08007)
* [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](https://arxiv.org/abs/2506.08889)
* [Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.01939)
* [Inference-Time Hyper-Scaling with KV Cache Compression](https://arxiv.org/abs/2506.05345)
* [MiMo-VL Technical Report](https://arxiv.org/abs/2506.03569)

## May 2025
* [KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction](https://arxiv.org/abs/2505.23416)
* [SageAttention2++: A More Efficient Implementation of SageAttention2](https://arxiv.org/abs/2505.21136v2)
* [QwenLong-CPRS: Towards $\infty$-LLMs with Dynamic Context Optimization](https://arxiv.org/abs/2505.18092v2)
* [Scaling Law for Quantization-Aware Training](https://arxiv.org/abs/2505.14302)
* [Quartet: Native FP4 Training Can Be Optimal for Large Language Models](https://arxiv.org/abs/2505.14669v2)
* [Emerging Properties in Unified Multimodal Pretraining](https://arxiv.org/abs/2505.14683v2)
* [SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training](https://arxiv.org/abs/2505.11594v1)
* [Chain-of-Model Learning for Language Model](https://arxiv.org/abs/2505.11820)
* [AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning](https://arxiv.org/abs/2505.11896)
* [Fast and Accurate Sparse Attention Inference by Delta Correction](https://arxiv.org/abs/2505.11254v1)
* [Model Merging in Pre-training of Large Language Models](https://arxiv.org/abs/2505.12082)
* [EfficientLLM: Efficiency in Large Language Models](https://arxiv.org/abs/2505.13840v1)
* [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388v1)
* [DanceGRPO: Unleashing GRPO on Visual Generation](https://arxiv.org/abs/2505.07818v1)
* [AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection](https://arxiv.org/abs/2505.07293)
* [MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining](https://arxiv.org/abs/2505.07608v2)
* [Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures](https://arxiv.org/abs/2505.09343v1)
* [Seed1.5-VL Technical Report](https://arxiv.org/abs/2505.07062v1)
* [Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/abs/2505.00949v3)
* [An Empirical Study of Qwen3 Quantization](https://arxiv.org/abs/2505.02214v1)
* [BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs](https://arxiv.org/abs/2504.18415v1)

## April 2025
* [TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives](https://arxiv.org/abs/2503.20313)
* [Multi-Token Attention](https://arxiv.org/abs/2504.00927)
* [BitNet b1.58 2B4T Technical Report](https://arxiv.org/abs/2504.12285v2)
* [Efficient Pretraining Length Scaling](https://arxiv.org/abs/2504.14992v2)
* [Hogwild! Inference: Parallel LLM Generation via Concurrent Attention](https://arxiv.org/abs/2504.06261v3)
* [Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models](https://arxiv.org/abs/2504.03624)

## March 2025
* [Qwen2.5-Omni Technical Report](https://arxiv.org/abs/2503.20215v1)
* [Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts](https://arxiv.org/abs/2502.19811)
* [Gemma 3 Technical Report](https://arxiv.org/abs/2503.19786v1)
* [Frac-Connections: Fractional Extension of Hyper-Connections](https://arxiv.org/html/2503.14125v1)
* [DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://arxiv.org/abs/2503.14476)
* [Pranjal Shankhdhar Outperforming cuBLAS on H100: a Worklog](https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog)
* [GitHub - bertmaher/simplegemm](https://github.com/bertmaher/simplegemm)
* [Training Video Foundation Models with NVIDIA NeMo](https://arxiv.org/abs/2503.12964v1)
* [Technologies on Effectiveness and Efficiency: A Survey of State Spaces Models](https://arxiv.org/abs/2503.11224v1)
* [A Review of DeepSeek Models' Key Innovative Techniques](https://arxiv.org/abs/2503.11486v1)
* [Transformers without Normalization](https://arxiv.org/abs/2503.10622v1)
* [Communication-Efficient Language Model Training Scales Reliably and Robustly: Scaling Laws for DiLoCo](https://arxiv.org/abs/2503.09799)
* [Quantization for OpenAI's Whisper Models: A Comparative Analysis](https://arxiv.org/abs/2503.09905v1)
* [Gemini Embedding: Generalizable Embeddings from Gemini](https://arxiv.org/abs/2503.07891v1)

## February 2025
* [Hardware-Aligned and Natively Trainable Sparse Attention](https://arxiv.org/abs/2502.11089)
* [Qwen2.5-VL Technical Report](https://arxiv.org/abs/2502.13923)
* [Memory-Efficient LoRA Training for Large Language Models](https://arxiv.org/abs/2502.13533)
* [Eager Updates For Overlapped Communication and Computation in DiLoCo](https://arxiv.org/abs/2502.12996)
* [Logical Reasoning in Large Language Models: A Survey](https://arxiv.org/abs/2502.09100)
* [InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU](https://arxiv.org/abs/2502.08910)
* [TransMLA: Multi-Head Latent Attention Is All You Need](https://arxiv.org/abs/2502.07864)
* [Matryoshka Quantization](https://arxiv.org/abs/2502.06786)
* [How To Scale Your Model](https://jax-ml.github.io/scaling-book/)
* [FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation](https://arxiv.org/abs/2502.01068)

## January 2025
* [s1: Simple test-time scaling](https://arxiv.org/abs/2501.19393)
* [Streaming DiLoCo with overlapping communication: Towards ](https://arxiv.org/abs/2501.18512)
* [Optimizing Large Language Model Training Using FP4 Quantization](https://arxiv.org/abs/2410.15163)
* [SFT Memorizes, RL Generalizes: A Comparative Study of Founda…](https://arxiv.org/abs/2501.17161)
* [Sigma: Differential Rescaling of Query, Key and Value for...](https://arxiv.org/abs/2501.13629)
* [Parameter-Efficient Fine-Tuning for Foundation Models](https://arxiv.org/abs/2403.14608)
* [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via.…](https://arxiv.org/abs/2501.12948)
* [Kimi k1.5: Scaling Reinforcement Learning with LLMs](https://arxiv.org/abs/2501.12599)
* [Foundations of Large Language Models](https://arxiv.org/abs/2501.09223)
* [MiniMax-01: Scaling Foundation Models with Lightning Attenti…](https://arxiv.org/abs/2501.08313)
* [Tensor Product Attention Is All You Need](https://arxiv.org/abs/2501.06425)
* [O1 Replication Journey -- Part 3: Inference-time Scaling for…](https://arxiv.org/abs/2410.18982)
* [Transformer-Squared: Self-adaptive LLMs](https://arxiv.org/abs/2501.06252)
* [PyTorch Forums [Distributed w/ TorchTitan] Breaking Barriers: Training Long…](https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-parallel/215082)
* [Scaling Laws for Floating Point Quantization Training](https://arxiv.org/abs/2501.02423)
* [Titans: Learning to Memorize at Test Time](https://arxiv.org/html/2501.00663v1)

## December 2024
* [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437)
* [1.58-bit FLUX](https://arxiv.org/abs/2412.18653)
* [OpenAI o1 System Card](https://cdn.openai.com/o1-system-card-20241205.pdf)
* [MixLLM: LLM Quantization with Global Mixed-precision between…](https://arxiv.org/abs/2412.14590)
* [Qwen2.5 Technical Report](https://arxiv.org/abs/2412.15115)
* [PyTorch Forums [Distributed w/ TorchTitan] Training with Zero-Bubble Pipeli…](https://discuss.pytorch.org/t/distributed-w-torchtitan-training-with-zero-bubble-pipeline-parallelism/214420)
* [No More Adam: Learning Rate Scaling at Initialization is All…](https://arxiv.org/abs/2412.11768)
* [Byte Latent Transformer: Patches Scale Better Than Tokens](https://arxiv.org/abs/2412.09871)
