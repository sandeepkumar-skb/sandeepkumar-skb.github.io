---
title: ML System Papers and Resources
date: 2025-06-08 
categories: [Deep Learing, ML Systems, ML Acceleration]
tags: [gpu, deep learning, ml_systems]     # TAG names should always be lowercase
---


# ML System Papers
##December 2025
[Qwen3-VL Technical Report](https://arxiv.org/abs/2511.21631)
[DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models](https://arxiv.org/abs/2512.02556)

## November 2025
[SAM 3: Segment Anything with Concepts](https://arxiv.org/abs/2511.16719)
[TiDAR: Think in Diffusion, Talk in Autoregression](https://arxiv.org/abs/2511.08923)

## October 2025
* [INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats](https://arxiv.org/abs/2510.25602)
* [Attention Is All You Need for KV Cache in Diffusion LLMs](https://arxiv.org/abs/2510.14973)
* [RL makes MLLMs see better than SFT](https://arxiv.org/abs/2510.16333)
* [MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation](https://arxiv.org/abs/2510.18692)
* [Attention Sinks in Diffusion Language Models](https://arxiv.org/abs/2510.15731)
* [Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning](https://arxiv.org/abs/2510.19338)
* [Long-Context Attention Benchmark: From Kernel Efficiency to Distributed Context Parallelism](https://arxiv.org/abs/2510.17896)
* [Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference](https://arxiv.org/abs/2510.18413)

## September 2025
* [A Survey of Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2509.08827)
* [Pretraining Large Language Models with NVFP4](https://arxiv.org/abs/2509.25149)
* [Qwen3-Omni Technical Report](https://arxiv.org/abs/2509.17765)
* [QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models](https://arxiv.org/abs/2509.17428)
* [Sharing is Caring: Efficient LM Post-Training with Collective RL Experience Sharing](https://arxiv.org/abs/2509.08721)
* [Causal Attention with Lookahead Keys](https://arxiv.org/abs/2509.07301)
* [Why Language Models Hallucinate](https://arxiv.org/abs/2509.04664)
* [The Landscape of Agentic Reinforcement Learning for LLMs: A Survey](https://arxiv.org/abs/2509.02547)

## August 2025
* [Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs](https://arxiv.org/abs/2508.14896)
* [NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model](https://arxiv.org/abs/2508.14444)
* [Memento: Fine-tuning LLM Agents without Fine-tuning LLMs](https://arxiv.org/abs/2508.16153)
* [TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling](https://arxiv.org/abs/2508.17445)
* [Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference](https://arxiv.org/abs/2508.19559)
* [TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill and Decode Inference](https://arxiv.org/abs/2508.15881)
* [Ovis2.5 Technical Report](https://arxiv.org/abs/2508.11737)
* [XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization](https://arxiv.org/abs/2508.10395)
* [Deep Researcher with Test-Time Diffusion](https://arxiv.org/abs/2507.16075)
* [Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts](https://arxiv.org/abs/2508.07785)
* [MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs](https://arxiv.org/abs/2508.05257)
* [Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference](https://arxiv.org/abs/2508.02193)
* [Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance](https://arxiv.org/abs/2507.22448)
* [Qwen-Image Technical Report](https://arxiv.org/abs/2508.02324)
* [Group Sequence Policy Optimization](https://arxiv.org/abs/2507.18071)

## July 2025
* [MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism](https://arxiv.org/abs/2504.02263)
* [Kimi K2: Open Agentic Intelligence](https://arxiv.org/abs/2507.20534)
* [A Survey on Latent Reasoning](https://arxiv.org/abs/2507.06203)
* [SingLoRA: Low Rank Adaptation Using a Single Matrix](https://arxiv.org/abs/2507.05566)
* [A Systematic Analysis of Hybrid Linear Attention](https://arxiv.org/abs/2507.06457)
* [Scaling RL to Long Videos](https://arxiv.org/abs/2507.07966)
* [Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities](https://arxiv.org/abs/2507.06261)
* [SPD: Sync-Point Drop for Efficient Tensor Parallelism of Large Language Models](https://arxiv.org/abs/2502.20727)
* [any4: Learned 4-bit Numeric Representation for LLMs](https://arxiv.org/abs/2507.04610)
* [AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs](https://arxiv.org/abs/2507.05687)
* [Fast and Simplex: 2-Simplicial Attention in Triton](https://arxiv.org/abs/2507.02754)
* [ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention](https://arxiv.org/abs/2507.01004)
* [AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training](https://arxiv.org/abs/2507.01663)
* [Kwai Keye-VL Technical Report](https://arxiv.org/abs/2507.01949)

## June 2025
* [Ovis-U1 Technical Report](https://arxiv.org/abs/2506.23044)
* [VMoBA: Mixture-of-Block Attention for Video Diffusion Models](https://arxiv.org/abs/2506.23858)
* [Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation](https://arxiv.org/abs/2506.19852)
* [GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization](https://arxiv.org/abs/2506.20807)
* [BestServe: Serving Strategies with Optimal Goodput in Collocation and Disaggregation Architectures](https://arxiv.org/abs/2506.05871) :fire:
* [Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity](https://arxiv.org/abs/2505.21411)
* [Truncated Proximal Policy Optimization](https://arxiv.org/abs/2506.15050)
* [MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models](https://arxiv.org/abs/2506.14435)
* [SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation](https://arxiv.org/abs/2506.18349)
* [CommVQ: Commutative Vector Quantization for KV Cache Compression](https://arxiv.org/abs/2506.18879)
* [DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster](https://arxiv.org/abs/2506.21263)
* [Scaling Speculative Decoding with Lookahead Reasoning](https://arxiv.org/abs/2506.19830)
* [OAgents: An Empirical Study of Building Effective Agents](https://arxiv.org/abs/2506.15741)
* [Efficient RL Training - Optimizing Memory Usage in verl](https://hebiao064.github.io/rl-memory-management)
* [FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving](https://arxiv.org/abs/2501.01005)
* [Hardware-Efficient Attention for Fast Decoding](https://arxiv.org/abs/2505.21487) :fire:
* [PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models](https://arxiv.org/abs/2506.16054)
* [Scaling Test-time Compute for LLM Agents](https://arxiv.org/abs/2506.12928)
* [MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention](https://arxiv.org/abs/2506.13585)
* [Magistral](https://arxiv.org/abs/2506.10910)
* [Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/abs/2506.06395)
* [Seedance 1.0: Exploring the Boundaries of Video Generation Models](https://arxiv.org/abs/2506.09113)
* [NoLoCo: No-all-reduce Low Communication Training Method for Large Models](https://arxiv.org/abs/2506.10911)
* [Reinforcement Pre-Training](https://arxiv.org/abs/2506.08007)
* [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](https://arxiv.org/abs/2506.08889)
* [Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.01939)
* [Inference-Time Hyper-Scaling with KV Cache Compression](https://arxiv.org/abs/2506.05345)
* [MiMo-VL Technical Report](https://arxiv.org/abs/2506.03569)

## May 2025
* [KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction](https://arxiv.org/abs/2505.23416)
* [SageAttention2++: A More Efficient Implementation of SageAttention2](https://arxiv.org/abs/2505.21136v2)
* [QwenLong-CPRS: Towards $\infty$-LLMs with Dynamic Context Optimization](https://arxiv.org/abs/2505.18092v2)
* [Scaling Law for Quantization-Aware Training](https://arxiv.org/abs/2505.14302)
* [Quartet: Native FP4 Training Can Be Optimal for Large Language Models](https://arxiv.org/abs/2505.14669v2)
* [Emerging Properties in Unified Multimodal Pretraining](https://arxiv.org/abs/2505.14683v2)
* [SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training](https://arxiv.org/abs/2505.11594v1)
* [Chain-of-Model Learning for Language Model](https://arxiv.org/abs/2505.11820)
* [AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning](https://arxiv.org/abs/2505.11896)
* [Fast and Accurate Sparse Attention Inference by Delta Correction](https://arxiv.org/abs/2505.11254v1)
* [Model Merging in Pre-training of Large Language Models](https://arxiv.org/abs/2505.12082)
* [EfficientLLM: Efficiency in Large Language Models](https://arxiv.org/abs/2505.13840v1)
* [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388v1)
* [DanceGRPO: Unleashing GRPO on Visual Generation](https://arxiv.org/abs/2505.07818v1)
* [AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection](https://arxiv.org/abs/2505.07293)
* [MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining](https://arxiv.org/abs/2505.07608v2)
* [Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures](https://arxiv.org/abs/2505.09343v1)
* [Seed1.5-VL Technical Report](https://arxiv.org/abs/2505.07062v1)
* [Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/abs/2505.00949v3)
* [An Empirical Study of Qwen3 Quantization](https://arxiv.org/abs/2505.02214v1)
* [BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs](https://arxiv.org/abs/2504.18415v1)

## April 2025
* [Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler](https://arxiv.org/abs/2504.19442)
* [TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives](https://arxiv.org/abs/2503.20313)
* [Multi-Token Attention](https://arxiv.org/abs/2504.00927)
* [BitNet b1.58 2B4T Technical Report](https://arxiv.org/abs/2504.12285v2)
* [Efficient Pretraining Length Scaling](https://arxiv.org/abs/2504.14992v2)
* [Hogwild! Inference: Parallel LLM Generation via Concurrent Attention](https://arxiv.org/abs/2504.06261v3)
* [Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models](https://arxiv.org/abs/2504.03624)

## March 2025
* [Qwen2.5-Omni Technical Report](https://arxiv.org/abs/2503.20215v1)
* [Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts](https://arxiv.org/abs/2502.19811)
* [Gemma 3 Technical Report](https://arxiv.org/abs/2503.19786v1)
* [Frac-Connections: Fractional Extension of Hyper-Connections](https://arxiv.org/html/2503.14125v1)
* [DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://arxiv.org/abs/2503.14476)
* [Pranjal Shankhdhar Outperforming cuBLAS on H100: a Worklog](https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog)
* [GitHub - bertmaher/simplegemm](https://github.com/bertmaher/simplegemm)
* [Training Video Foundation Models with NVIDIA NeMo](https://arxiv.org/abs/2503.12964v1)
* [Technologies on Effectiveness and Efficiency: A Survey of State Spaces Models](https://arxiv.org/abs/2503.11224v1)
* [A Review of DeepSeek Models' Key Innovative Techniques](https://arxiv.org/abs/2503.11486v1)
* [Transformers without Normalization](https://arxiv.org/abs/2503.10622v1)
* [Communication-Efficient Language Model Training Scales Reliably and Robustly: Scaling Laws for DiLoCo](https://arxiv.org/abs/2503.09799)
* [Quantization for OpenAI's Whisper Models: A Comparative Analysis](https://arxiv.org/abs/2503.09905v1)
* [Gemini Embedding: Generalizable Embeddings from Gemini](https://arxiv.org/abs/2503.07891v1)

## February 2025
* [Hardware-Aligned and Natively Trainable Sparse Attention](https://arxiv.org/abs/2502.11089)
* [Qwen2.5-VL Technical Report](https://arxiv.org/abs/2502.13923)
* [Memory-Efficient LoRA Training for Large Language Models](https://arxiv.org/abs/2502.13533)
* [Eager Updates For Overlapped Communication and Computation in DiLoCo](https://arxiv.org/abs/2502.12996)
* [Logical Reasoning in Large Language Models: A Survey](https://arxiv.org/abs/2502.09100)
* [InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU](https://arxiv.org/abs/2502.08910)
* [TransMLA: Multi-Head Latent Attention Is All You Need](https://arxiv.org/abs/2502.07864)
* [Matryoshka Quantization](https://arxiv.org/abs/2502.06786)
* [How To Scale Your Model](https://jax-ml.github.io/scaling-book/)
* [FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation](https://arxiv.org/abs/2502.01068)

## January 2025
* [s1: Simple test-time scaling](https://arxiv.org/abs/2501.19393)
* [Streaming DiLoCo with overlapping communication: Towards ](https://arxiv.org/abs/2501.18512)
* [Optimizing Large Language Model Training Using FP4 Quantization](https://arxiv.org/abs/2410.15163)
* [SFT Memorizes, RL Generalizes: A Comparative Study of Founda…](https://arxiv.org/abs/2501.17161)
* [Sigma: Differential Rescaling of Query, Key and Value for...](https://arxiv.org/abs/2501.13629)
* [Parameter-Efficient Fine-Tuning for Foundation Models](https://arxiv.org/abs/2403.14608)
* [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via.…](https://arxiv.org/abs/2501.12948)
* [Kimi k1.5: Scaling Reinforcement Learning with LLMs](https://arxiv.org/abs/2501.12599)
* [Foundations of Large Language Models](https://arxiv.org/abs/2501.09223)
* [MiniMax-01: Scaling Foundation Models with Lightning Attenti…](https://arxiv.org/abs/2501.08313)
* [Tensor Product Attention Is All You Need](https://arxiv.org/abs/2501.06425)
* [O1 Replication Journey -- Part 3: Inference-time Scaling for…](https://arxiv.org/abs/2410.18982)
* [Transformer-Squared: Self-adaptive LLMs](https://arxiv.org/abs/2501.06252)
* [PyTorch Forums [Distributed w/ TorchTitan] Breaking Barriers: Training Long…](https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-parallel/215082)
* [Scaling Laws for Floating Point Quantization Training](https://arxiv.org/abs/2501.02423)
* [Titans: Learning to Memorize at Test Time](https://arxiv.org/html/2501.00663v1)

## December 2024
* [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437)
* [1.58-bit FLUX](https://arxiv.org/abs/2412.18653)
* [OpenAI o1 System Card](https://cdn.openai.com/o1-system-card-20241205.pdf)
* [MixLLM: LLM Quantization with Global Mixed-precision between…](https://arxiv.org/abs/2412.14590)
* [Qwen2.5 Technical Report](https://arxiv.org/abs/2412.15115)
* [PyTorch Forums [Distributed w/ TorchTitan] Training with Zero-Bubble Pipeli…](https://discuss.pytorch.org/t/distributed-w-torchtitan-training-with-zero-bubble-pipeline-parallelism/214420)
* [No More Adam: Learning Rate Scaling at Initialization is All…](https://arxiv.org/abs/2412.11768)
* [Byte Latent Transformer: Patches Scale Better Than Tokens](https://arxiv.org/abs/2412.09871)
