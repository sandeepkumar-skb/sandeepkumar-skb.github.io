# ML System Papers and Resources

## May 2025

* [SageAttention2++: A More Efficient Implementation of SageAttention2](http://arxiv.org/abs/2505.21136v2)
* [QwenLong-CPRS: Towards $\infty$-LLMs with Dynamic Context Optimization](http://arxiv.org/abs/2505.18092v2)
* [Scaling Law for Quantization-Aware Training](https://arxiv.org/abs/2505.14302)
* [Quartet: Native FP4 Training Can Be Optimal for Large Language Models](http://arxiv.org/abs/2505.14669v2)
* [Emerging Properties in Unified Multimodal Pretraining](http://arxiv.org/abs/2505.14683v2)
* [SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training](http://arxiv.org/abs/2505.11594v1)
* [Chain-of-Model Learning for Language Model](https://arxiv.org/abs/2505.11820)
* [AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning](https://arxiv.org/abs/2505.11896)
* [Fast and Accurate Sparse Attention Inference by Delta Correction](http://arxiv.org/abs/2505.11254v1)
* [Model Merging in Pre-training of Large Language Models](https://arxiv.org/abs/2505.12082)
* [EfficientLLM: Efficiency in Large Language Models](http://arxiv.org/abs/2505.13840v1)
* [Qwen3 Technical Report](http://arxiv.org/abs/2505.09388v1)
* [DanceGRPO: Unleashing GRPO on Visual Generation](http://arxiv.org/abs/2505.07818v1)
* [AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection](https://arxiv.org/abs/2505.07293)
* [MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining](http://arxiv.org/abs/2505.07608v2)
* [Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures](http://arxiv.org/abs/2505.09343v1)
* [Seed1.5-VL Technical Report](http://arxiv.org/abs/2505.07062v1)
* [Llama-Nemotron: Efficient Reasoning Models](http://arxiv.org/abs/2505.00949v3)
* [An Empirical Study of Qwen3 Quantization](http://arxiv.org/abs/2505.02214v1)
* [BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs](http://arxiv.org/abs/2504.18415v1)

## April 2025
* [TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives](https://arxiv.org/abs/2503.20313)
* [Multi-Token Attention](https://arxiv.org/abs/2504.00927)
* [BitNet b1.58 2B4T Technical Report](http://arxiv.org/abs/2504.12285v2)
* [Efficient Pretraining Length Scaling](http://arxiv.org/abs/2504.14992v2)
* [Hogwild! Inference: Parallel LLM Generation via Concurrent Attention](http://arxiv.org/abs/2504.06261v3)
* [Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models](https://arxiv.org/abs/2504.03624)

## March 2025
* [Qwen2.5-Omni Technical Report](http://arxiv.org/abs/2503.20215v1)
* [Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts](https://arxiv.org/abs/2502.19811)
* [Gemma 3 Technical Report](http://arxiv.org/abs/2503.19786v1)
* [Frac-Connections: Fractional Extension of Hyper-Connections](https://arxiv.org/html/2503.14125v1)
* [DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://arxiv.org/abs/2503.14476)
* [Pranjal Shankhdhar Outperforming cuBLAS on H100: a Worklog](https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog)
* [GitHub - bertmaher/simplegemm](https://github.com/bertmaher/simplegemm)
* [Training Video Foundation Models with NVIDIA NeMo](http://arxiv.org/abs/2503.12964v1)
* [Technologies on Effectiveness and Efficiency: A Survey of State Spaces Models](http://arxiv.org/abs/2503.11224v1)
* [A Review of DeepSeek Models' Key Innovative Techniques](http://arxiv.org/abs/2503.11486v1)
* [Transformers without Normalization](http://arxiv.org/abs/2503.10622v1)
* [Communication-Efficient Language Model Training Scales Reliably and Robustly: Scaling Laws for DiLoCo](https://arxiv.org/abs/2503.09799)
* [Quantization for OpenAI's Whisper Models: A Comparative Analysis](http://arxiv.org/abs/2503.09905v1)
* [Gemini Embedding: Generalizable Embeddings from Gemini](http://arxiv.org/abs/2503.07891v1)

## February 2025
* [Hardware-Aligned and Natively Trainable Sparse Attention](https://arxiv.org/abs/2502.11089)
* [Qwen2.5-VL Technical Report](https://arxiv.org/abs/2502.13923)
* [Memory-Efficient LoRA Training for Large Language Models](https://arxiv.org/abs/2502.13533)
* [Eager Updates For Overlapped Communication and Computation in DiLoCo](https://arxiv.org/abs/2502.12996)
* [Logical Reasoning in Large Language Models: A Survey](https://arxiv.org/abs/2502.09100)
* [InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU](https://arxiv.org/abs/2502.08910)
* [TransMLA: Multi-Head Latent Attention Is All You Need](https://arxiv.org/abs/2502.07864)
* [Matryoshka Quantization](https://arxiv.org/abs/2502.06786)
* [How To Scale Your Model](https://jax-ml.github.io/scaling-book/)
* [FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation](https://arxiv.org/abs/2502.01068)

## January 2025
* [s1: Simple test-time scaling](https://arxiv.org/abs/2501.19393)
* [Streaming DiLoCo with overlapping communication: Towards ](https://arxiv.org/abs/2501.18512)
* [Optimizing Large Language Model Training Using FP4 Quantization](https://arxiv.org/abs/2410.15163)
* [SFT Memorizes, RL Generalizes: A Comparative Study of Founda…](https://arxiv.org/abs/2501.17161)
* [Sigma: Differential Rescaling of Query, Key and Value for...](https://arxiv.org/abs/2501.13629)
* [Parameter-Efficient Fine-Tuning for Foundation Models](https://arxiv.org/abs/2403.14608)
* [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via.…](https://arxiv.org/abs/2501.12948)
* [Kimi k1.5: Scaling Reinforcement Learning with LLMs](https://arxiv.org/abs/2501.12599)
* [Foundations of Large Language Models](https://arxiv.org/abs/2501.09223)
* [MiniMax-01: Scaling Foundation Models with Lightning Attenti…](https://arxiv.org/abs/2501.08313)
* [Tensor Product Attention Is All You Need](https://arxiv.org/abs/2501.06425)
* [O1 Replication Journey -- Part 3: Inference-time Scaling for…](https://arxiv.org/abs/2410.18982)
* [Transformer-Squared: Self-adaptive LLMs](https://arxiv.org/abs/2501.06252)
* [PyTorch Forums [Distributed w/ TorchTitan] Breaking Barriers: Training Long…](https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-parallel/215082)
* [Scaling Laws for Floating Point Quantization Training](https://arxiv.org/abs/2501.02423)
* [Titans: Learning to Memorize at Test Time](https://arxiv.org/html/2501.00663v1)

## December 2024
* [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437)
* [1.58-bit FLUX](https://arxiv.org/abs/2412.18653)
* [OpenAI o1 System Card](https://cdn.openai.com/o1-system-card-20241205.pdf)
* [MixLLM: LLM Quantization with Global Mixed-precision between…](https://arxiv.org/abs/2412.14590)
* [Qwen2.5 Technical Report](https://arxiv.org/abs/2412.15115)
* [PyTorch Forums [Distributed w/ TorchTitan] Training with Zero-Bubble Pipeli…](https://discuss.pytorch.org/t/distributed-w-torchtitan-training-with-zero-bubble-pipeline-parallelism/214420)
* [No More Adam: Learning Rate Scaling at Initialization is All…](https://arxiv.org/abs/2412.11768)
* [Byte Latent Transformer: Patches Scale Better Than Tokens](https://arxiv.org/abs/2412.09871)
